---
alias: LLMParserLLMbasedLogParsingFramework2023
tags: unread
rating: ⭐
share: false
ptype: article
---

# LLMParser: A LLM-based Log Parsing Framework
<cite>* Authors: [[Zhihan Jiang]], [[Jinyang Liu]], [[Zhuangbin Chen]], [[Yichen Li]], [[Junjie Huang]], [[Yintong Huo]], [[Pinjia He]], [[Jiazhen Gu]], [[Michael R. Lyu]]</cite>


* [Local library](zotero://select/items/1_X8VIM79A)

## Abstract

The process of log parsing, which converts log messages into structured formats, is a crucial step for various log analysis tasks. Although numerous log parsers have been proposed, their effectiveness on complex log data is often hindered due to reliance on human-made rules or learning-based models with limited training data. The recent rise of powerful large language models (LLMs) shows potential for log parsing due to their extensive pre-trained knowledge related to code and logging. However, their accuracy is currently limited due to the lack of specialized log parsing capabilities. Additionally, the inconsistency of their answers and significant overhead obstruct the practical implementation of LLM-based log parsing. To tackle these challenges, we introduce LLMParser, the first practical LLM-based log parsing framework. LLMParser enables accurate and robust log parsing by leveraging the in-context learning (ICL) capability of the LLM, employing a hierarchical candidate sampling algorithm, and selecting high-quality demonstrations. LLMParser also includes a novel adaptive parsing cache component to store and refine the templates generated by the LLM. This design aids in addressing the inefficiency of LLMs by rapid matching to previously parsed log templates. LLMParser also adaptively updates the templates in the parsing cache to ensure consistent parsed results. Extensive evaluation on large-scale public datasets demonstrates that LLMParser surpasses the state-of-the-art methods. Furthermore, LLMParser significantly reduces the query times to LLMs, achieving efficiency comparable to the most efficient baseline, Drain.


***

### 初读印象

comment:: 


