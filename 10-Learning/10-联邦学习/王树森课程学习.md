https://www.youtube.com/watch?v=STxtRucv_zo&ab_channel=ShusenWang

两个应用
+ Google 希望从用户移动设备中的数据训练数据
+ 多个医院希望用他们的数据训练数据 

问题都在于隐私

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/7a53d994f7394b12a2c04fa2a8b9b103-20221019212533-73d247.png)
可以使用参数服务器，Data wont leave work nodes

### 联邦学习 

本质仍然是分布式机器学习

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/35e4ae81c66d2478b9eb0a86ba085df7-20221019212650-3c57a5.png)

**和传统分布式机器学习区别点**

1. 用户有对 device 和 data 的控制 
2. **worker node 不稳定**
3. **通信代价远大于计算代价**
4. 每个 worker node 的数据并非**并非独立同分布** (not IID)：很多已有的优化算法不适用了
5. 数据量严重不平衡

这些问题导致建模和计算很难。由于 2 和 3 两个原因，设计FL算法最重要的是减少通信次数。哪怕让计算量大很大很多 

### 研究方向1 Communication-Efficiency

Trade computation for communication.

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/6ea9d254694017d22c11b5c35450fa1a-20221019213922-79e35d.png)
图中表示手机可以在充电时进行计算

这个研究方向的基本想法是：尽量减少迭代次数

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/6b43f5147bc46b871842f097adcab0dd-20221019215913-cf2463.png)

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/bc0d87e113c23d5a7787b25c6ce27f76-20221019220020-618d75.png)

>一般的梯度下降，不会进行多次epoch，只进行一次

这样做可以在两次通信之间进行多次改进。

#### Computation vs. Communication 

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/ea77d4156caa02ad75c2473c5ad44db4-20221019220055-fdeac1.png)

相同次数的通信，FedAvg 可以更快收敛。

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/82b2bbebdad9ce28efdf46bccb63e218-20221019220250-8a7caa.png)

相同次数 epochs ，FedAvg 更慢

#### 相关文献

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/47adec7594d7415f1a45e6367c9bdf92-20221019220520-eb02be.png)

#### Communication-Efficient 算法

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/2e3bd7ee1e35b088ec8fe495396cf927-20221019221049-9af0d5.png)
1. McMahan, Moore, Ramage, Hampson, & Arcas. Communication-efficient learning of deep networks  from decentralized data. In AISTATS, 2017.  
2. Stich. Local SGD converges fast and communicates little. In ICLR, 2018.  
3. Li, Sahu, Talwalkar, & Smith. Federated optimization in heterogeneous networks. arXiv, 2018.  
4. Wang & Joshi. Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms. arXiv, 2018.  
5. Fan & Cong. On the convergence properties of a k-step averaging stochasticgradient descent  algorithm for nonconvex optimization. In IJCAI, 2018.  
6. Lin, Stich, and Jaggi. Don’t use large mini-batches, use local SGD. arXiv, 2018.  
7. Li, Huang, Yang, Wang, & Zhang. On the convergence of FedAvg on non-IID data. arXiv, 2019.  
8. Khaled, Mishchenko, Richtárik. First analysis of local GD on heterogeneous data. arXiv, 2019.  
9. Yu, Yang, Zhu. Parallel restarted SGD with faster convergence and less communication: Demystifying  why model averaging works for deep learning. In AAAI, 2019.

### 研究方向2：防止隐私泄露

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/19/01497ae1bdccdc8674b64e3d57e434b7-20221019221112-4f5aed.png)
1. Hitaj, Ateniese, & Perez-Cruz. Deep models under the GAN: information leakage from collaborative  deep learning. In ACM SIGSAC Conference on Computer and Communications Security, 2017.  
2. Melis, Song, Cristofaro, & Shmatikov. Exploiting unintended feature leakage in collaborative learning.  In IEEE Symposium on Security & Privacy, 2019.  
3. Zhu, Liu, & Han. Deep leakage from gradients. In NIPS, 2019.  
4. Orekondy, Oh, Zhang, Schiele, & Fritz. Gradient-Leaks: Understanding and controlling deanonymization in federated learning. arXiv, 2018.  
5. Ateniese, Felici, Mancini, Spognardi, Villani, & Vitali. Hacking smart machines with smarter ones:  How to extract meaningful data from machine learning classifiers. International Journal of Security  and Networks, 2015.  
6. Fredrikson, Jha, & Ristenpart. Model inversion attacks that exploit confidence information and basic  countermeasures. In CCS, 2015.  
7. Ganju, Wang, Yang, Gunter, & Borisov. Property Inference Attacks on Fully Connected Neural  Networks using Permutation Invariant Representations. In CCS, 2018.  
8. Jia, Salem, Backes, Zhang, & Gong. Property inference attacks on fully connected neural networks  using permutation invariant representations. In CCS, 2019.

### 研究方向3：对抗鲁棒性

拜占庭将军问题，模型下毒

1. Shafah, Huang, Najibi, Suciu, Studer, Dumitras, Goldstein. Poison frogs! targeted clean-label  poisoning attacks on neural networks. In NIPS, 2018.  
2. Bhagoji, Chakraborty, Mittal, & Calo. Analyzing federated learning through an adversarial lens. In  ICML, 2019.  
3. Koh, Steinhardt, & Liang. Stronger data poisoning attacks break data sanitization defenses. arXiv,  2018.  
4. Fang, Cao, Jia, & Gong. Local model poisoning attacks to Byzantine-robust federated learning. arXiv,  2019.  
5. Blanchard, Guerraoui, & Stainer. Machine learning with adversaries: Byzantine tolerant gradient  descent. In NIPS, 2017.  
6. Chen, Su, & Xu. Distributed statistical machine learning in adversarial settings: Byzantine gradient  descent. In Proceedings of the ACM on Measurement and Analysis of Computing Systems, 2017.  
7. Yin, Chen, Ramchandran, Bartlett. Byzantine-robust distributed learning: Towards optimal statistical  rates. In ICML, 2018.
