
### 云云场景中的横向联邦

具有相同数据结构的多个参与者通过参数或云服务器协同建立机器学习模型。假设**参与者是诚实**的，**服务器是好奇**的，因此不允许任何参与者向服务器泄露信息。通常包括以下四个步骤 

1. **参与者在本地计算训练梯度**，使用加密、差异隐私或秘密共享技术掩饰所选梯度，并将掩码后的结果发送到服务器；
2. **服务器执行安全聚合**，不了解任何参与者的信息；
3. **服务器将汇总后的结果（新的参数）发送给参与者**；
4. 参与者用解密的梯度**更新他们各自的模型**。

由于**训练节点不稳定**和**通信代价大**，联邦学习不同于传统分布式学习，联邦学习不能在每次单步训练后，同步不同训练节点上的权重。

---

### 提高*计算通信比*：FebAvg

谷歌公司在2017年 [fedavg]提出了联邦平均算法(Federated Averaging，FedAvg)

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/10/20/6a11e42a12c3308813c24ffe02f7121e-20221020114321-5c842e.png)

---

#### FebAvg 不适用的场景

1. 数据异质性（端上数据不是 I.I.D 分布，即独立同分布）
2. 系统异质性（端设备时断时连）

在这种情况下，对模型参数只进行简单的**加权平均**会在训练过程中**引入大量偏差**，影响收敛速度，预测性能

---

### 端云场景中的横向联邦

端云联邦的总体流程和云云联邦一样，但端云联邦学习面临的难点还包括以下三个方面：

1. **高昂的通信代价**。在联邦学习问题中，原始数据保存在远程客户端设备本地，必须与中央服务器不断交互才能完成全局模型的构建。通常的通信网络可能是WLAN或移动数据，网络通信速度可能比本地计算慢许多个数量级，这就造成高昂的通信代价成为了联邦学习的关键瓶颈。
2. **系统异质性**。由于客户端设备硬件条件（CPU、内存）、网络连接（3G、4G、5G、WIFI）和电源（电池电量）的变化，联邦学习网络中每个设备的存储、计算和通信能力都有可能不同。网络和设备本身的限制可能导致某一时间仅有一部分设备处于活动状态。此外，设备还会出现没电、网络无法接入等突发状况，导致瞬时无法连通。这种异质性的系统架构影响了联邦学习整体策略的制定。
3. **隐私问题**。联邦学习共享客户端设备中的模型参数更新（例如梯度信息）而不是原始数据，因此在数据隐私保护方面优于其他的分布式学习方法。然而，在训练过程中传递模型的更新信息仍然存在向第三方或中央服务器暴露敏感信息的风险。隐私保护成为联邦学习需要重点考虑的问题。

