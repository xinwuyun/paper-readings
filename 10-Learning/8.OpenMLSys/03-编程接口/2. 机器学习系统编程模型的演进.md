![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/09/17/cfd0a4aab3679c7cb117eca35c9849b9-20220917171756-097334.png)
https://openmlsys.github.io/chapter_programming_interface/development_history.html

### 早期框架

事实 
+ 机器学习系统诞生，如何设计易用且高性能的编程接口就一直成为了框架设计者首要解决的问题

Lua(Torch) and Python(Theano)
提供模型定义、自动微分等功能。

### DNN 崛起
2011 年
事实
+ 深度神经网络快速崛起，并很快在各个AI应用领域（计算机视觉，语音识别，自然语言处理等）取得了最先进的性能 
+ 训练它需要大量算力，而这些算力无法被以Lua和Python所主导开发的Torch和Theano所满足
+ 加速器的通用编程接口日趋成熟（英伟达的 CUDA），CPU 多核多线程。

框架
+ 直接用 C/C++，使用 Caffe 框架 

问题 
+ 不是所有人都会用 C/C++ 搞 AI

### TensorFlow 出现 
2016 Google 推出 TensorFlow 
Python作为面向用户的主要前端语言，。而利用C和C++实现高性能后端

TensorFlow兼有Python的灵活性和生态，同时也通过C/C++后端得以实现高性能。这种设计在日后崛起的PyTorch，MXNet和CNTK的机器学习框架得到传承。

### 多种其他机器学习框架出现

随着多个机器学习框架的出现，Keras和TensorLayer等高层次机器学习开发库提供了更高层次的Python API从而可以快速导入已有的模型， 这些高层次API进一步屏蔽了底层框架的实现细节，因此Keras和TensorLayer可以运行在不同的机器学习框架之上。

2020 年后， Mindspore JAX 出现。

### 分布式

同时，超大型数据集和超大型深度神经网络崛起让分布式执行成为了机器学习框架编程模型的核心设计需求。为了实现分布式执行，TensorFlow和PyTorch的使用者需要进行大量编程来将数据集和神经网络分配到分布式节点上，而大量的AI开发人员并不具有分布式编程的能力。因此MindSpore进一步完善了机器学习框架的分布式编程模型的能力，从而让单节点的MindSpore程序可以无缝地运行在海量节点上。
