https://openmlsys.github.io/chapter_programming_interface/neural_network_layer.html
随着深度神经网络的飞速发展，各种深度神经网路结构层出不穷。

### 构建深度神经网络结构最基本的规则 

1. 承载计算的节点
2. 可变化的节点权重（节点权重可训练）
3. 允许数据流动的结点连接

**机器学习编程库中**神经网络以**层**为核心，提供各类神经网络层基本组件；将神经网络层组件按照网络结构进行堆叠，连接就能构造出神经网络模型

### 以层为核心定义神经网络

+ 卷积 Convolution
+ 池化 Pooling
+ 全连接 Fully Connected
+ 循环神经网络 Recurrent Neural Network [[RNN笔记]] 
+ BatchNorm, Dropout, 用来防止过拟合

使用卷积、池化、全连接组件就可以构建一个非常简单的卷积神经网络了
![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/09/19/687100c8cc93d1b58873267a01a1f227-20220919204023-572bb3.png)
伪代码描述如下

```python
# 构建卷积神经网络的组件接口定义：
全连接层接口：fully_connected(input, weights)
卷积层的接口：convolution(input, filters, stride, padding)
最大池化接口：pooling(input, pool_size, stride, padding, mode='max')
平均池化接口：pooling(input, pool_size, stride, padding, mode='mean')

# 构建卷积神经网络描述：
input:(3,64,64)大小的图片
# 创建卷积模型的训练变量,使用随机数初始化变量值
conv1_filters = variable(random(size=(3, 3, 3, 16)))
conv2_filters = variable(random(size=(3, 3, 16, 32)))
fc1_weights = variable(random(size=(8192, 128)))
fc2_weights = variable(random(size=(128, 64)))
fc3_weights = variable(random(size=(64, 10)))
# 将所有需要训练的参数收集起来
all_weights = [conv1_filters, conv2_filters, fc1_weights, fc2_weights, fc3_weights]

# 构建卷积模型的连接过程
output = convolution(input, conv1_filters, stride=1, padding='same')
output = pooling(output, kernel_size=3, stride=2, padding='same', mode='max')
output = convolution(output, conv2_filters, stride=1, padding='same')
output = pooling(output, kernel_size=3, stride=2, padding='same', mode='max')
output = flatten(output)
output = fully_connected(output, fc1_weights)
output = fully_connected(output, fc2_weights)
output = fully_connected(output, fc3_weights)
```

随着神经网络发展，诞生了丰富的组件。比如 RNN、LSTM、Encoder、Decoder 和 Attention 层等等。

### 层的实现原理

上面的伪代码是比较底层的代码，训练变量和构建连接都是手动的。

大部分框架都提供了更高级友好的 API，如MindSpore提供的`mindspore.nn.Cell`、`mindspore.nn.Conv2d`、`mindspore.dataset`； PyTorch提供的`torch.nn.Module`、`torch.nn.Conv2d`、`torch.utils.data.Dataset`。

![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/09/19/4ab8a64f43b674f2c0da7e1088f9c45a-20220919210408-9a7742.png)



**神经网络层**需要的功能有
+ 训练参数
	+ 变量，包括初始化方法和训练状态
	#TODO 什么意思？
+ 计算过程 

**神经网络模型**需要的功能是对神经网络层管理和神经网络层参数的管理


![](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/09/20/db497d10bb65908f7a86d83d43efb3cd-20220920101430-96f21f.png)
神经网络基类抽象方法

### 自定义神经网络层

假设已经有了神经网络模型抽象方法Cell，
+ 构建 Conv2D 将继承 Cell
+ 重构 `__init__` 和 `__call__` 方法，
+ 在 `__init__` 里初始化训练参数和输入参数
+ 在`__call__`里调用低级API实现计算逻辑。同样使用伪代码接口描述自定义卷积层的过程。
```python
# 接口定义 
# 卷积层的接口：convolution(input, fliter, stride, padding)
# 变量：Variable(value, trainable=True)
# 高斯分布初始化方法：random_normal(shape)
# 神经网络模型抽象方法：Cell 

class Conv2D(Cell):
	def __init__(self, in_channels, out_channels, ksize, stride, padding):
		# ksize: 卷积核大小 
		filters_shape = (out_channels, in_channels, ksize, ksize)
		self.stride = stride
		self.padding = padding 
		self.filters = Variable(random_normal(filters_shape))

	def __call__(self, inputs):
		outputs = convolution(inputs, self.filters, self.stride, self.padding)

```

### 自定义神经网络模型

神经网络层是 Cell 的子类实现。

神经网络模型也是 Cell 的子类，多个神经网络层可以认为是一个大层。只需要修改 `__call__` method 即可 

```python
# 使用Cell子类构建的神经网络层接口定义：
# 构建卷积神经网络的组件接口定义：
全连接层接口：Dense(in_channel, out_channel)
卷积层的接口：Conv2D(in_channel, out_channel, filter_size, stride, padding)
最大池化接口：MaxPool2D(pool_size, stride, padding)
张量平铺：Flatten()

# 使用SubClass方式构建卷积模型
class CNN(Cell):
    def __init__(self):
        self.conv1 = Conv2D(in_channel=3, out_channel=16, filter_size=3, stride=1, padding=0)
        self.maxpool1 = MaxPool2D(pool_size=3, stride=1, padding=0)
        self.conv2 = Conv2D(in_channel=16, out_channel=32, filter_size=3, stride=1, padding=0)
        self.maxpool2 = MaxPool2D(pool_size=3, stride=1, padding=0)
        self.flatten = Flatten()
        self.dense1 = Dense(in_channels=768, out_channel=128)
        self.dense2 = Dense(in_channels=128, out_channel=64)
        self.dense3 = Dense(in_channels=64, out_channel=10)

    def __call__(self, inputs):
        z = self.conv1(inputs)
        z = self.maxpool1(z)
        z = self.conv2(z)
        z = self.maxpool2(z)
        z = self.flatten(z)
        z = self.dense1(z)
        z = self.dense2(z)
        z = self.dense3(z)
        return z
net = CNN()
```
从`__init__`开始，第一个是 `Conv2D`，他也是继承自 `Cell`，会进入到Conv2D的`__init__`，此时会将第一个Conv2D的卷积参数收集到`self._params`，之后回到Conv2D，将第一个Conv2D收集到`self._cells`


>依次类推，分别收集第二个卷积参数和卷积层，三个全连接层的参数和全连接层。实例化之后可以调用net.parameters_and_names来返回训练参数；调用net.cells_and_names查看神经网络层列表。

### 3.4 C/C++编程接口

Ptyhon与C/C++有良好的互操作性。用户可以添加自定义算子来帮助实现新的模型、优化器和数据处理函数。

### 3.4.2 [添加自定义算子](https://openmlsys.github.io/chapter_programming_interface/c_python_interaction.html#c)
...看原文

大概三个步骤

1. Primitive 注册
2. GPU kernel 实现
3. GPU kernel 注册 

# 3.5. [总结](https://openmlsys.github.io/chapter_programming_interface/summary.html#id1 "Permalink to this headline")

-   现代机器学习系统需要兼有易用性和高性能，因此其一般选择Python作为前端编程语言，而使用C和C++作为后端编程语言。
    
-   一个机器学习框架需要对一个完整的机器学习应用工作流进行编程支持。这些编程支持一般通过提供高层次Python API来实现。
    
-   数据处理编程接口允许用户下载，导入和预处理数据集。
    
-   模型定义编程接口允许用户定义和导入机器学习模型。
    
-   损失函数接口允许用户定义损失函数来评估当前模型性能。同时，优化器接口允许用户定义和导入优化算法来基于损失函数计算梯度。
    
-   机器学习框架同时兼有高层次Python API来对训练过程，模型测试和调试进行支持。
    
-   复杂的深度神经网络可以通过叠加神经网络层来完成。
    
-   用户可以通过Python API定义神经网络层，并指定神经网络层之间的拓扑来定义深度神经网络。
    
-   Python和C之间的互操作性一般通过CType等技术实现。
    
-   机器学习框架一般具有多种C和C++接口允许用户定义和注册C++实现的算子。这些算子使得用户可以开发高性能模型，数据处理函数，优化器等一系列框架拓展。
    

# 3.6. [扩展阅读](https://openmlsys.github.io/chapter_programming_interface/summary.html#id2 "Permalink to this headline")

-   MindSpore编程指南：[MindSpore](https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.6/index.html)
    
-   Python和C/C++混合编程：[Pybind11](https://pybind11.readthedocs.io/en/latest/basics.html#creating-bindings-for-a-simple-function)