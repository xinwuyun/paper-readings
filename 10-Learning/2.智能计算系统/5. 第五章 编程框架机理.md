# 第五章 编程框架机理
#AI #AISystem #tensorflow 

+ TensorFlow设计原则 
	+ 高性能、易开发、可移植
+ TensorFlow计算图机制
	+ 自动求导
	+ 计算图本地执行
	+ 分布式执行
+ TensorFlow系统实现
	+ 整体架构、执行模块、设备管理、网络和通信、算子实现、
+ 驱动范例：以VGG19为例介绍TensorFlow内部的实现机理
- 编程框架对比 ：
	- TensorFlow、PyTorch、Caffe、MXNet框架的对比

## 一、Tensorflow 设计原则 

### 1. 高性能

![[Pasted image 20220812161604.png]]

> 普通应用开发者可以受益于这些算子 
> 计算图也有一系列优化，归并，提炼，剪枝等
> 可以并行，例如图中 c 和 d 没有依赖关系，可以并发执行

### 2. 易开发

+ 可以用于异构系统
+ 每个算子提供了在不同设备不同底层上的实现

## 二、TensorFlow 计算图机制

### 1. 计算图的自动求导

+ 深度学习通常采用**梯度下降法**更新模型参数
+ <font color=#ED7001> 大部分深度学习框架提供自动梯度计算功能</font>
+ 用户只需描述前向计算的过程，由变成框架自动推导反向计算图，完成导数计算

**常用求导方法**
1. 反向传播法：手动利用链式法则求出梯度公式 
	1. 缺点：![[Pasted image 20220812163003.png]]
2. 数值求导法 ![[Pasted image 20220812163613.png]]
	1. 优点：易操作，对用户隐藏求解过程
	2. 计算量大，求解慢、可能引起舍入误差和阶段误差
3.  符号求导法：
	1. 缺点：表达式膨胀
	![[Pasted image 20220812164252.png]]
4. **自动求导法**：![[Pasted image 20220812164401.png]]
![[Pasted image 20220812164443.png]]

> 自动求导法非常适合于计算图结构。

Tensorflow 会自动生成对应的反向计算节点

![[Pasted image 20220812164558.png]]![[Pasted image 20220812164709.png]]

**总结**

+ 计算分两步进行
	1. 原始函数建立计算图，数据正常传播，计算出中间节点 $x_i$ ，记录节点依赖关系 
	2. 反向遍历计算图，计算输出对于每个节点的导数 
![[Pasted image 20220812164919.png]]
![[Pasted image 20220812165549.png]]

#### 对比
![[Pasted image 20220812165625.png]]
### 2. 检查点机制

模型训练过程中可以用 `tf.train.Saver()` 保存模型中的所有变
![[Pasted image 20220812165731.png#center|保存模型]]
![[Pasted image 20220812165758.png#center|恢复模型]]
#### 2.1 原理

![[Pasted image 20220812165821.png]]

+ 通过向计算图中插入 Save 节点及关联节点来保存模型 
+ 恢复模型也是插入 Restore 节点及关联节点来恢复模型 

### 3. TensorFlow 中的控制流

+ 使用**控制流算子**来实现不同复杂的控制流场景
+ 引入少量的**简单基础操作**，可以为 tf 应用提供丰富的控制流表达
+ 每一个操作都会在一个**执行帧**中被执行，**控制流操作**负责管理和创建这些执行帧

![[Pasted image 20220812170821.png]]

![[Pasted image 20220812170946.png]]![[Pasted image 20220812171207.png]]
#### 条件表达式
![[Pasted image 20220812171809.png#center|条件表达式]]

![[Pasted image 20220812172204.png#center|条件表达式代码实现]]
#### 循环操作 

![[Pasted image 20220812172308.png]]
![[Pasted image 20220812172331.png]]

### 4. 计算图的执行模型

+ client：通过 session 接口与 master 和 worker  通信。worker 可以是多个
+ master：控制worker 执行计算图
+ worker：每个 worker 负责一个或多个计算设备的仲裁访问，根据 master 的指令，执行这些计算设备的计算图节点
+ 设备：可以是CPU或GPU等

#### 简单示例

![[Pasted image 20220812175031.png]]
> 这是client和master之间的关系，master 会把计算结果返回给 client，

![[Pasted image 20220812180616.png]]
本地分两种。1. 一个GPU；2. 多个GPU
分布式：多个 worker，每个 worker 可能有多个 GPU

#### 本地单设备进行

![[Pasted image 20220812201546.png]]

#### 本地多设备进行（*数据并行*）

+ **CPU 作为参数服务器**，用户保存参数和变量、计算梯度平均等。
+ **GPU作为 worker**，用于模型训练
1. 本地将数据切分为一个一个 batch
2. 数据分别送到多个 GPU 进行模型训练，每个 GPU 分到不同数据
3. **每个 GPU 分别训练，求 loss 得到梯度**，把梯度送回 CPU 进行模型平均 
4. CPU 接收 GPU 传来的梯度加和求平均，更新参数 
5. GPU 更新参数 
6. 重复 2-5 直到模型收敛

![[Pasted image 20220812221012.png]]

[[@Li2014]]

#### 分布式执行

> 深度学习数据量越来越大，一台机器搞不定。就需要这个

+ client、master和worker可以工作于不同机器上的进程中
+ 兼容本地多设备执行模式

![[Pasted image 20220812223625.png]]

### 5. 计算图本地执行

1. 计算图剪枝：得到最小计算图，去掉无关节点 
2. 计算图分配：可以根据设备约束分配各个节点的device
3. 计算图优化：tf会进行一系列优化提升计算图的运行效率
4. 计算图切分：切分子图，每个设备会创建自身的计算子图

#### 1. 计算图剪枝

+ 目的：得到本地运行的最小子图

![[Pasted image 20220812224124.png]]
> de节点可以丢掉


##### 为输入输出建立与外界的交互

![[Pasted image 20220812224209.png]]

#### 去除与最终输出节点无关的节点和边
![[Pasted image 20220813002050.png]]
这样被裁剪的图就是一个完整的联通的大图，而不是独立的子图

### 2. 计算图分配

**多设备**运行环境中，对计算图中的每个节点**如何分配计算设备**
目的：**保证计算的快速进行**

![[Pasted image 20220813002218.png]]

使用 cost model。
![[Pasted image 20220813002252.png]]
### 3. 计算图优化

TF 中的图优化由 ***Grappler*** 实现

![[Pasted image 20220813002407.png]]
+ 可以根据**硬件结构**调整计算策略，从而获得**更快速度和更高硬件利用率**
+ 减少**峰值内存**，

**主要方法**

1. ConstFold：常量折叠
2. Arithmetic：算术优化
3. Layout：布局优化
4. Remapper： 算子融合

#### 常量折叠

![[Pasted image 20220813002635.png]]
> mul和concat例子都是减少了计算

#### 算术优化 

![[Pasted image 20220813002816.png]]

#### 3. 布局优化 
> 说白了就是数据摆放

![[Pasted image 20220813002851.png]]

#### 4. 重映射
![[Pasted image 20220813003058.png]]![[Pasted image 20220813003058 1.png]]
### 5. 计算图切分和设备通信 

完成每个节点的设备分配后，将整个计算图按照分配设备分成若干**子图**，每个设备一张子图。

**对于跨设备通信的边**：
![[Pasted image 20220813003246.png]]
1. 将跨设备的边删除
2. 设备便捷插入 send 和 recv 节点 
3. 在设备 A 对应的子图中，增加 x 节点到 send 节点的边
4. 在设备 B 对应的子图中，增加 recv 节点到 y 结点的边
5. 插入节点时。规定单个设备上特定张量的多个用户使用单个 recv 节点。如 b 和 c。从而确保所需的张量在设备之间只传输一次
6. 执行计算图时，通过 send 和 recv 节点来实现跨设备数据传输

### 6. 计算图的分布式执行

背景：神经网络规模及数据规模指数级增加
分布式技术：讲一个大的神经网络模型，拆分成许多小的部分，同时分配在多个节点上计算。

> 目前主流深度学习框架均支持分布式技术

#### 分布式通信

+ 点到点通信 Point to Point
+ 集合通信  Collective 

TF 实现了集合通信的基本算子 
![[Pasted image 20220813003839.png]]

#### 容错机制

+ TF 中有一些错误检查和容错机制
+ send 和 recv 节点会检查传输的正确性
+ 出错时，计算图执行过程会停止并重启
+ TF 训练过程中会有检查点，可以立即恢复

## 三、TensorFlow 系统实现

### 1.  整体架构
![[Pasted image 20220813004152.png]]

+ 面向各个语言的语言包
+ C/C++ API
+ 后端代码
	+ Session 
	+ 图优化和切分 
	+ 本地运行时
	+ 分布式运行时实现 
	+ 算子库（针对不同设备定制开发）

### 2. 计算图执行模块（Session）

![[Pasted image 20220813004327.png]]

> 每个设备会有一个 executor ，负责本设备子计算图的执行

![[Pasted image 20220813004423.png]]![[Pasted image 20220813004509.png#center|计算图执行前的传参过程]]
![[Pasted image 20220813004533.png]]![[Pasted image 20220813004607.png#center|RunInternal函数]]
**执行器逻辑**

![[Pasted image 20220813004638.png]]

> 有计算依赖的放到同一个流里，无依赖的放到不同流，尽量减少 send recv 

**SceduleReady 逻辑流程**

![[Pasted image 20220813004803.png]]

> 高开销用新线程执行

![[Pasted image 20220813005203.png]]

### 3. 设备抽象和管理

+ TF 将设备分为：本地设备和远程设备。不同设备调不同的底层库
+ TF 使用注册机制来管理设备。每个设备负责一个子图的运算，可通过**注册接口**支持自定义设备
+ ***DeviceBase类***：定义了基本的数据结构与接口
+ ***LocalDevice类***
+ 本地设备基于 localDevice 类创建自己的设备类，然后注册。

![[Pasted image 20220813005528.png#center|BaseDLPDevice]]

> 给出了各种属性和方法。

### 4. 网络和通信

+ 设备间通信：**Send 和 Recv**。使用 **Rendezvous 机制**（汇合）完成数据交互 
+ Rendezvous 机制对外提供了 Send、 Recv、RecvAsync 接口和实现。**不同通信场景下需要不同的实现**
+ 本地传输：LocalRendezvous
+ 跨进程：RemoteRendezvous（RPC机制）


+ 每个 Rendezvous 实例拥有一个通道表，记录个每对 send recv 的关系和状态
+ 生产者用 send 方法传输到特定通道，消费者使用 Receive 方法从特定通道中获取数据。消费者可以在任意时刻调用 Receive 方法获取数据，可以选择使用**回调或者阻塞方法**获取数据 

![[Pasted image 20220813010000.png]]

> CreateKey 构造键值，ParsedKey 解析键值 

![[Pasted image 20220813010134.png]]

![[Pasted image 20220813010223.png]]
> 具体的 Send  RecvAsync 方法由派生类实现

> RecvAsync 是个异步接收函数，Recv 同步接收是用 RecvAsync 封装出来的

**本地通信 LocalRendezvous **
![[Pasted image 20220813010338.png]]

**远程通信**
![[Pasted image 20220813010358.png]]
![[Pasted image 20220813010415.png]]
![[Pasted image 20220813010505.png]]

### 5. 算子实现

+ **算子是 TF 的基本单元**，OpKernel 是算子的特定执行，依赖底层硬件。
+ TensorFlow 通过**注册机制**来支持不同的算子和相应的 **OpKernel 函数**

![[Pasted image 20220813010631.png#center|三种设备的对应OpKernel(第三个是寒武纪)]]

![[Pasted image 20220813010708.png]]

**示例**
![[Pasted image 20220813010734.png]]

注册时需要
1. 名字
2. 设备类型
3. 数据类型 
4. OpKernel 对象（真正干活的东西 算子）

## 四、驱动范例 

![[Pasted image 20220813010916.png]]
![[Pasted image 20220813010926.png]]
![[Pasted image 20220813010945.png]]
![[Pasted image 20220813010959.png]]
![[Pasted image 20220813011011.png]]
![[Pasted image 20220813011033.png]]

> 顺序： 
>1. 创建执行器
>2. 创建函数**调用帧**
>3. 执行器并行执行（真正到硬件设备上运行）

## 编程框架对比

![[Pasted image 20220813011239.png]]

![[Pasted image 20220813011252.png]]

![[Pasted image 20220813011624.png]]

![[Pasted image 20220813011647.png]]
