# 神经网络基础
#神经网络 #深度学习 
神经网络是一种机器学习算法，经过70多年的发展，逐渐成为人工智能的主流。例如，本课的驱动范例——图像风格迁移，一般就是基于神经网络实现的。本章首先从线性回归开始，逐步介绍机器学习和神经网络的基本原理，然后介绍神经网络的训练过程，以及提升神经网络训练精度的一些手段，最后介绍神经网络的交叉验证方法。

![[Pasted image 20220810103430.png]]

## 从机器学习到神经网络

![[Pasted image 20220810103227.png#center|人工智能包含关系]]

### 机器学习相关概念

+ **机器学习**是对能通过经验自动改进的计算机算法的研究
+ **机器学习**是用数据或以往的经验，提升计算机程序的能力
+ **机器学习**是研究如何通过计算的手段、利用经验来改善系统自身性能的一门学科

![[Pasted image 20220810104504.png#center|典型机器学习过程]]
### 符号说明

输入数据： $x$
真实值 (实际值) : $y$
计算值 (模型输出值)： $\hat{y}$
模型函数： $H(x)$
激活函数:    $G(x)$
损失函数:$\quad L(x)$
标量：斜体小写字母 $a 、 b 、 c$
向量:   黑斜体小写字母 $a 、 b 、 c$
矩阵：黑斜体大写字母 $A 、 B 、 C$


## 线性回归

>什么是回归？什么是线性回归？
>回归是一种用统计方法分析一组变量和因变量的关系。
>线性回归可以找到一些点的集合背后的规律，一个点集可以用一条直线来拟合 

单变量线形模型：$H_{w}(x)=w_{0}+w x$

![[Pasted image 20220810111343.png]]

多变量线形回归模型

$$
\begin{gathered}
H_{w}(x)=w_{0}+w_{1} x_{1}+w_{2} x_{2} \\
H_{w}(x)=\sum_{i=0}^{n} w_{i} x_{i}=\widehat{\boldsymbol{w}}^{T} \boldsymbol{x} \\
\widehat{\boldsymbol{w}}=\left[w_{0} ; w_{1} ; \ldots w_{n}\right] \\
\boldsymbol{x}=\left[x_{o} ; x_{1} ; \ldots x_{n}\right], x_{0}=1
\end{gathered}
$$

### 损失函数

模型预测值 $\hat{y}$ 与真实值之间存在误差：$\varepsilon=y-\hat{y}=y-\widehat{\boldsymbol{w}}^{T} \boldsymbol{x}$

$\varepsilon$ 满足 $\mathrm{N}\left(0, \sigma^{2}\right)$ 的高斯分布 

 $p(\varepsilon)=\frac{1}{\sqrt{2 \pi \sigma}} \exp \left(-\frac{(\varepsilon)^{2}}{2 \sigma^{2}}\right)$
$$
p(y \mid x ; \widehat{\boldsymbol{w}})=\frac{1}{\sqrt{2 \pi \sigma}} \exp \left(-\frac{\left(y-\widehat{\boldsymbol{w}}^{T} \boldsymbol{x}\right)^{2}}{2 \sigma^{2}}\right)
$$
通过极大似然函数，得到损失函数 

$L(\widehat{w})=\frac{1}{2} \sum_{j=1}^{m}\left(H_{w}\left(\boldsymbol{x}_{j}\right)-y_{j}\right)^{2}=\frac{1}{2} \sum_{j=1}^{m}\left(\widehat{w}^{T} x-y_{j}\right)^{2}$

目标：求出参数 $\hat{w}$ 使得损失函数 $L(\hat{w})$ 最小。

### 梯度下降法

> 也可以用最小二乘法 

![[Pasted image 20220810114337.png]]

## 人工神经网络 

![[Pasted image 20220810114605.png]]


> 反向传播算法很重要

### 人工神经元

![[Pasted image 20220810114909.png#center|人工神经元]]

包含输入输出和计算功能的模型。

> 生物神经元：人工神经元 = 老鼠：米老鼠 

### 感知机模型（最基础的神经网络模型）

$H(\boldsymbol{x})=\operatorname{sign}\left(\boldsymbol{w}^{T} \boldsymbol{x}+b\right)$

![[Pasted image 20220810115138.png]]

> 线性激活函数
> $\operatorname{sign}(x)=\left\{\begin{array}{l}+1, x>0 \\-1, x<0\end{array}\right.$

考虑数据集$\mathrm{D}=\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right), \ldots,\left(\boldsymbol{x}_{m}, y_{m}\right)\right\}$，其中，$x_j\in \text{R}_n,y_j\in \{+1,-1\}$

求超平面

$$
\mathrm{S}:(\boldsymbol{w}^{T} \boldsymbol{x}+b=0)
$$

使得正负样本分到 S 两侧。**策略**：误分类的点到超平面的总距离（欧氏距离）最短

![[Pasted image 20220810120925.png]]

### 随机梯度下降下降 
![[Pasted image 20220810122723.png]]


## 两层神经网络-多层感知机 

### 浅层神经网络特点

需要数据量小、训练快。但是对于复杂函数的表示能力有限，针对复杂分类问题，其泛化能力收到制约。

<font color=#FF0211>我们需要更深层的。</font>

## 多层神经网络（深度学习）

![[Pasted image 20220810152614.png]]

![[Pasted image 20220810152627.png#center|深度神经网络的成功——ABC]]

+ 随着网络层数的增加，每一层对于前一层次的抽象表示更深入
+ 通过抽取更抽象的特征来对食物进行区分，从而获得更好的区分与分类能力
![[Pasted image 20220810152940.png]]

## 神经网络的训练

![[Pasted image 20220810153207.png#center|神经网络训练]]

> 正向传播：求解的过程
> 反向传播：看参数对 $L({w})$ 的影响

### 正向传播

![[Pasted image 20220810154124.png]]
![[Pasted image 20220810154135.png]]
![[Pasted image 20220810154148.png]]

最后计算出输出$\hat{y}$与期望结果还有差距，需要用反向传播修改权重。

### 反向传播

1. 损失函数 

![[Pasted image 20220810163659.png]]

2. **根据偏导数的链式法则推导**

![[Pasted image 20220810173925.png]]

3. 更新$w_{2,1}^{(2)}$的权重

![[Pasted image 20220810174157.png]]

> 梯度下降法

## 神经网络设计原则
> 训练完了测不准怎么办

有三种东西可以调整（超参数比如学习率也可以）
+ 激活函数
+ 损失函数
+ 拓扑结构
	+ 神经元个数
	+ 隐层设计（AutoML）

### 激活函数 

必须有两个特征
+ **可微性**
+ **输出值的范围**，如果输出值有限，基于梯度的优化方法会更稳定

##### sigmoid

![[Pasted image 20220810211646.png]]

存在问题：
1. 始终是正的
2. 计算机进行*指数运算*速度很慢（芯片角度）
3. 梯度消失和饱和性问题（当x非常大时，梯度很小，收敛会很缓慢）

##### 替代品

![[Pasted image 20220810212012.png]]

![[Pasted image 20220810212040.png]]
![[Pasted image 20220810212335.png]]![[Pasted image 20220810212350.png]]

### 选择恰当的损失函数

**损失函数的特性**
1. 损失函数不是唯一的
2. 损失函数是 w和b 的函数 
3. 损失函数可以评价模型的好坏，损失函数越小说明模型和参数越符合训练样本
4. 损失函数是一个标量 
5. 损失函数要对参数 w 和 b 可微性 
6. <font color=#777777>损失函数也叫代价函数、目标函数</font>


#### 均方差损失函数

![[Pasted image 20220810212524.png]]
> 使用sigmoid和均方差损失函数可能出现梯度消失问题，导致参数更新缓慢  

#### 交叉熵损失函数

能解决上面的问题

$$
L=-\frac{1}{m} \sum_{x \in D} \sum_{i} y_{i} \ln \left(\hat{y}_{i}\right)
$$
m为训练样本总数量，i为分类类别

![[Pasted image 20220810212721.png]]


## 过拟合和正则化 

![[Pasted image 20220810213050.png#center|过拟合和欠拟合]]
> **他们看起来是这样的：**
> 欠拟合：训练特征太少，拟合函数误差较大
> 过拟合：训练集上太过完美，**但是泛化能力差**，把一些局部特征当做全体特征 

### 解决正则化的方法：正则化
#正则化 #过拟合
**过拟合overfitting**正式定义：模型过度接近训练的数据，模型泛化能力不足。表现为训练集上误差很低，验证数据集上误差很大

+ 神经网络层数层架，表示能力大幅增强的情况下容易出现过拟合。

**解决的方法**：参数范数惩罚、稀疏化、Bagging集成、Dropout、提前终止、数据集扩增等正则化方法可以有效抑制过拟合。

#### 高次项惩罚项
![[Pasted image 20220810215151.png]]
![[Pasted image 20220810215256.png]]
![[Pasted image 20220810215310.png]]

#### 区间导数过大：L2正则化

![[Pasted image 20220810215507.png]]

#### L1正则化

![[Pasted image 20220810215527.png]]

## 交叉验证 
#交叉验证
> 过拟合某种意义上是人们过于沉溺于训练 

给每个样本作为测试集和训练集的机会
![[Pasted image 20220810220017.png#center|最简单的验证方法]]
![[Pasted image 20220810220428.png]]

## 小结
![[Pasted image 20220810220459.png]]